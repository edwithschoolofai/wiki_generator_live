{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데모\n",
    "#위키 문서로부터 학습하는 텐서플로우의 LSTM RNN 을 사용해 \n",
    "#새로운 데이터 세트 https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/ 를 생성합니다\n",
    "#위키피디아 기사로부터 1억 개의 표시를 불러오고 , MetaMind bought by Salesforce\n",
    "#big companies are buying AI startups like candy\n",
    "#우리는 케라스 대신 텐서플로우의 TF 를 사용하고\n",
    "#LSTM 의 내장함수를 사용합니다\n",
    "\n",
    "#테스트 입력 문자열과 500 개의 문자를 출력합니다\n",
    "\n",
    "import numpy as np #벡터화\n",
    "import random #확률분포를 생성합니다 \n",
    "import tensorflow as tf #ml\n",
    "import datetime #클록 훈련 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('text length in number of characters:', 1290590)\n",
      "head of text:\n",
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
      " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 20\n"
     ]
    }
   ],
   "source": [
    "#텍스트를 열어봅시다\n",
    "#파이썬 내장 파일 읽기 함수\n",
    "text = open('wiki.test.raw').read()\n",
    "print('text length in number of characters:', len(text))\n",
    "\n",
    "print('head of text:')\n",
    "print(text[:1000]) #표식이 붙은 단어로 text 라는 리스트에 저장되어 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of characters:', 178)\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x80', '\\x81', '\\x82', '\\x83', '\\x84', '\\x85', '\\x86', '\\x87', '\\x88', '\\x89', '\\x8a', '\\x8b', '\\x8c', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9b', '\\x9c', '\\x9d', '\\x9e', '\\x9f', '\\xa0', '\\xa1', '\\xa2', '\\xa3', '\\xa4', '\\xa5', '\\xa6', '\\xa7', '\\xa8', '\\xa9', '\\xaa', '\\xab', '\\xad', '\\xae', '\\xaf', '\\xb0', '\\xb1', '\\xb2', '\\xb3', '\\xb4', '\\xb5', '\\xb6', '\\xb7', '\\xb8', '\\xb9', '\\xba', '\\xbb', '\\xbc', '\\xbd', '\\xbe', '\\xbf', '\\xc2', '\\xc3', '\\xc4', '\\xc5', '\\xc7', '\\xc9', '\\xca', '\\xcb', '\\xcc', '\\xcd', '\\xce', '\\xcf', '\\xd0', '\\xd7', '\\xd8', '\\xd9', '\\xda', '\\xe0', '\\xe1', '\\xe2', '\\xe3', '\\xe4', '\\xe5', '\\xe6', '\\xe7', '\\xe8', '\\xe9']\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "#세트는 반복되는 요소가 없는 정렬되지 않은 것입니다.\n",
    "#리스트로 변환한 뒤, 모든 문자에 대해 \n",
    "#알파벳 순서로 정렬합니다\n",
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('number of characters:', char_size)\n",
    "print(chars)\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자를 id 로, id 를 문자로 변환하는\n",
    "#문자를 숫자로 숫자를 문자로 매핑하는 딕셔너리\n",
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))\n",
    "#print(id2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 문자의 확률을 가지고 원-핫 인코딩된 가장 확률이 높은 문자를 반환합니다\n",
    "#예측값으로 각 문자의 확률을 가진 배열을 알 수 있고\n",
    "#그 중에서 가장 확률이 높은 것을 원-핫 인코딩합니다\n",
    "def sample(prediction):\n",
    "    #샘플은 반열린구간 내에 균일 분포되어있습니다\n",
    "    r = random.uniform(0,1)\n",
    "    #예상 문자를 저장합니다\n",
    "    s = 0\n",
    "    #길이 > 지수 이기 때문에 0 부터 시작합니다\n",
    "    char_id = len(prediction) - 1\n",
    "    #각 문자별 예상 확률\n",
    "    for i in range(len(prediction)):\n",
    "        #S 에 할당합니다\n",
    "        s += prediction[i]\n",
    "        #확률이 무작위로 생성된 것보다 큰지 확인합니다\n",
    "        if s >= r:\n",
    "            #만약 그렇다면 그것이 다음 문자입니다\n",
    "            char_id = i\n",
    "            break\n",
    "    #dont try to rank, just differentiate \n",
    "    #initialize the vector 벡터를 초기화합니다\n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    #인코딩된 문자 ID\n",
    "    #https://image.slidesharecdn.com/latin-150313140222-conversion-gate01/95/representation-learning-of-vectors-of-words-and-phrases-5-638.jpg?cb=1426255492\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#모델에 맞게 데이터를 벡터화합니다\n",
    "\n",
    "len_per_section = 50\n",
    "skip = 2\n",
    "sections = []\n",
    "next_chars = []\n",
    "#리스트를 문자로 채웁니다.\n",
    "#문자 단계에서 실행하는 것이기 때문에\n",
    "#2 개의 문자마다 50 개의 문자를 생성합니다\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "#입력과 출력을 벡터화합니다\n",
    "#섹션의 길이와 문자의 수로 구성된 행렬\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "# 문자 ID 가 여전히 0 인 열을 찾아냅니다\n",
    "y = np.zeros((len(sections), char_size))\n",
    "#섹션의 각 문자마다 ID 로 변환합니다\n",
    "#각 섹션마다 라벨을 ID 로 변환합니다\n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "('training data size:', 645270)\n",
      "('approximate steps per epoch:', 1260)\n"
     ]
    }
   ],
   "source": [
    "#배치의 크기가 신경망을 거칠 샘플의 수를 결정합니다\n",
    "#1 epoch = 모든 훈련 샘플의 한 번의 순방향과 역방향 과정\n",
    "#배치의 크기 = 하나의 순방향과 역방향 과정에서의 훈련 샘플의 수\n",
    "#배치의 크기가 클 수록 필요한 메모리 공간이 많습니다\n",
    "#1000 개의 훈련 샘플이 있고\n",
    "#500 의 배치 크기를 가진다면, 1 epoch 을 마치려면 2 번의 반복이 필요합니다\n",
    "batch_size = 512\n",
    "#총 반복 수\n",
    "max_steps = 72001\n",
    "#로그 기록 주기\n",
    "log_every = 100\n",
    "#저장 주기\n",
    "save_every = 6000\n",
    "#은닉층에 너무 적은 수의 뉴런이 있다면 과소적합이 발생해\n",
    "#데이터 세트의 신호를 적절히 감지하지 못할 수 있습니다\n",
    "#너무 많다면 과대적합이 발생합니다\n",
    "hidden_nodes = 1024\n",
    "#시작문\n",
    "test_start = 'I am thinking that'\n",
    "#모델을 저장합니다\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "#체크포인트 디렉토리를 생성합니다\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#모델 시간을 구축합니다\n",
    "#계산 그래프를 생성합니다\n",
    "graph = tf.Graph()\n",
    "#다수의 그래프가 있으면 사용되지만, 여기서는 하나만 있습니다\n",
    "with graph.as_default():\n",
    "    ###########\n",
    "    #준비\n",
    "    ###########\n",
    "    #변수와 플레이스홀더\n",
    "    #global_step 은 그래프에 나타난 배치의 수를 의미합니다\n",
    "    #배치가 제공될 때마다 가중치는\n",
    "    #손실을 최소화하는 방향으로 갱신됩니다.\n",
    "    #global_step 은 0 부터 시작해 발견된 배치 수를 따라갑니다.\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    #텐서를 섹션에 적용합니다\n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    #라벨\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    #LSTM RNN (Long Short Term Memory) 는 3 개의 게이트와 내부 상태로 이루어져 있습니다. \n",
    "    #이것으로 LSTM 이 장기 종속성을 발견할 수 있습니다.\n",
    "    #http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n",
    "    #셀 상태와 3 개의 게이트에 대한 평중과 가중치를 생성해봅시다.\n",
    "    \n",
    "    #tf Variable\n",
    "    #모델에 대한 가중치와 편중이 필요하기 때문에\n",
    "    #이것을 마치 추가적인 입력으로 처리한다고 볼 수도 있습니다.\n",
    "    #하지만 텐서플로우에서는 Variable을 사용하는 것이 더 나을 수 있습니다.\n",
    "    #Variable 은 텐서플로우의 상호 연산 그래프에 있는 수정 가능한 텐서입니다\n",
    "    #계산에 사용될 수도 있고, 계산에 의해 수정될 수도 있습니다.\n",
    "    #머신러닝 응용프로그램에서는 매개변수를 Variable 로 사용합니다.\n",
    "    \n",
    "    #예비 LSTM 연산\n",
    "    #입력 게이트: 입력에 대한 가중치, 이전 출력에 대한 가중치, 편중\n",
    "    \n",
    "    #tf truncated normal\n",
    "    #절단 정규분포로부터 무작위값을 출력합니다.\n",
    "    #생성된 값은 지정된 정규분포의 평균과 표준편차를 가지지만\n",
    "    #평균으로부터 2 표준편차 이상 떨어져있는 값들은\n",
    "    #탈락시킵니다.\n",
    "    #무작위로 초기화된 값들입니다.\n",
    "    \n",
    "    #b편중값이 닻과 같은 역할을 합니다\n",
    "\n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #망각 게이트: 입력에 대한 가중치, 이전 출력값에 대한 가중치, 편중\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #출력 게이트: 입력에 대한 가중치, 이전 출력값에 대한 가중치, 편중\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #기억 셀: 입력에 대한 가중치, 이전 출력값에 대한 가중치, 편중\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    \n",
    "    #LSTM 셀\n",
    "    #입력값, 출력값과 외부 상태를 받아 출력과 상태를 반환합니다\n",
    "    #출력은 빈 상태로 시작하고 LSTM 셀이 계산하게 됩니다.\n",
    "    \n",
    "    #내부와 외부 두 개의 상태가 있고\n",
    "    #두 가지가 모두 있어야만\n",
    "    #이후의 연산을 이어나갈 수 있기 때문에\n",
    "    #각 과정에서 그들을 텐서로 결합하고 다음 과정의 입력으로 전달합니다\n",
    "    #이 텐서들을 각 과정의 처음 부분에 st_1 과 ct_1 로 풀어집니다\n",
    "    \n",
    "    \n",
    "    def lstm(i, o, state):\n",
    "        \n",
    "        #이것들은 개별적으로 계산되어서 서로 중복되지 않습니다\n",
    "        #(입력값 * 입력값의 가중치) + (출력값 * 이전 출력값의 가중치) + 편중\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        #(입력값 * 망각 가중치) + (출력값 * 이전 출력값의 가중치) + 편중\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        #(입력값 * 출력값의 가중치) + (출력값 * 이전 출력값의 가중치) + 편중\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        #(입력값 * 내부 상태의 가중치) + (출력값 * 이전 출력값의 가중치) + 편중\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        #망각 게이트 * 주어진 상태 +  입력 게이트 * 은닉 상태\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        #비선형 tanh 로 상태값을 압축합니다 (x 원소에 대해 쌍곡탄젠트값을 계산합니다)\n",
    "        #출력값을 곱해줍니다\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        #반환합니다\n",
    "        return output, state\n",
    "    \n",
    "    ###########\n",
    "    #연산\n",
    "    ###########\n",
    "    #LSTM\n",
    "    #빈 상태로 시작하고 LSTM 이 계산을 하게 됩니다\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    #LSTM 루프를 풀어줍니다\n",
    "    #각각의 입력 세트마다 실행해줍니다\n",
    "    for i in range(len_per_section):\n",
    "        #LSTM 으로부터 상태와 출력을 계산합니다\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #시작\n",
    "        if i == 0:\n",
    "            #초기 출력과 라벨을 저장합니다\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #각각의 새로운 세트마다 출력과 라벨을 이어줍니다\n",
    "        elif i != len_per_section - 1:\n",
    "            #차원축 위의 벡터를 이어서 결합해줍니다\n",
    "            outputs_all_i = tf.concat(0, [outputs_all_i, output])\n",
    "            labels_all_i = tf.concat(0, [labels_all_i, data[:, i+1, :]])\n",
    "        else:\n",
    "            #최종 저장\n",
    "            outputs_all_i = tf.concat(0, [outputs_all_i, output])\n",
    "            labels_all_i = tf.concat(0, [labels_all_i, labels])\n",
    "        \n",
    "    #Classifier\n",
    "    #The Classifier 는 saved_output 과 saved_state 이 지정되고 난 후에만 실행됩니다.\n",
    "    \n",
    "    #무작위의 크기와 분포로 생성된 신경망의\n",
    "    #가중치와 편중을 계산합니다.\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    #로지트는 단순히 함수가 이전 층의 출력값에 실행된다는 것을 의미하며\n",
    "    #단위를 이해하기 위한 상대적 비율은 선형입니다.\n",
    "    #그 말인 즉슨, 입력값이 확률이 아니기 때문에\n",
    "    #합이 1 이 아닐 수 있습니다. (5 의 입력값을 가집니다)\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    #로지트는 예상 출력입니다.\n",
    "    #다중클래스 분급기 소프트맥스 층의 비용과\n",
    "    #텐서의 차원 내 요소들의 평균을 계산하기 때문에\n",
    "    #이것을 교차 엔트로피와 비교해봐야 합니다.\n",
    "    #모든 값의 평균 손실\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_all_i))\n",
    "\n",
    "    #최적화\n",
    "    #경사 하강법을 이용한 손실을 최소화하고, 10 의 학습 속도를 가지며, 배치를 기억합니다.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #테스트\n",
    "    ###########\n",
    "    #test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    #test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #각 테스트의 시작 단계마다 재설정해줍니다\n",
    "    #reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                #test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    #test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    #test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 5.15 (2017-03-21 15:46:13.367470)\n",
      "================================================================================\n",
      "I am thinking that        e                                                                                                                                  e                                                                                                                                                                                                                                                                                                                                                                        \n",
      "================================================================================\n",
      "training loss at step 10: 6.73 (2017-03-21 15:48:13.244305)\n",
      "training loss at step 20: 3.22 (2017-03-21 15:50:01.814675)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-455f4f11e89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_add\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#그래프로 세션을 초기화합니다\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #표준 초기화 과정\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #매 훈련 과정마다 반복해줍니다\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #0 으로 시작합니다\n",
    "        offset = offset % len(X)\n",
    "        \n",
    "        #배치 데이터와 라벨을 계산해 모델에 적용합니다\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            #첫 단계\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #offset = batch_size 일 때까지 실행하고, 그 다음은\n",
    "        else:\n",
    "            #마지막 단계\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #최적화!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_start = 'I plan to make the world a better place '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #그래프를 초기화하고, 모델을 불러옵니다\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #입력 변수를 설정해 문자를 생성합니다\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #입력 문장의 모든 문자마다 반복\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #빈 문자 저장공간을 초기화합니다\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #아래로부터 가져와 id 로 저장합니다\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #모델에 적용하면, test_prediction 이 출력값이 됩니다\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #인코딩된 예상 문자를 저장하는 곳입니다\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #500 개의 문자를 생성해봅시다\n",
    "    for i in range(500):\n",
    "        #각 예상값의 확률을 구합니다\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        #원-핫 인코딩을 실행합니다\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        #최대값(최고 확률값)를 찾고 문자로 변환합니다\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #각 문자를 출력 글에 추가해줍니다\n",
    "        test_generated += next_char\n",
    "        #갱신해줍니다\n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
